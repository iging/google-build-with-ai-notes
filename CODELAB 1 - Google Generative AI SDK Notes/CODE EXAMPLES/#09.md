# Example 9: Model Parameter Configuration

This example demonstrates how to configure model parameters to control response generation behavior.

```python
from google.genai.types import GenerateContentConfig
from IPython.display import Markdown, display

# Generate content with customized parameters
response = client.models.generate_content(
    model=MODEL_ID,
    contents="Tell me how the internet works, but pretend I'm a puppy who only understands squeaky toys.",
    config=GenerateContentConfig(
        # Core generation parameters
        temperature=0.4,        # Controls randomness (lower = more focused)
        top_p=0.95,             # Nucleus sampling threshold
        top_k=20,               # Limits token selection to top K choices

        # Output control
        candidate_count=1,      # Number of alternative responses
        seed=5,                 # Random seed for reproducibility
        max_output_tokens=100,  # Maximum response length
        stop_sequences=["STOP!"],  # Sequences that stop generation

        # Repetition control
        presence_penalty=0.0,   # Reduces token repetition
        frequency_penalty=0.0,  # Reduces frequent tokens from prompt
    ),
)

# Display the result
print("Model response (limited to 100 tokens):")
print(response.text)

# Format the output as markdown
display(Markdown("**Formatted Response:**"))
display(Markdown(response.text))
```

## Explanation

This code demonstrates how to fine-tune the model's generation behavior using various parameters:

1. **Core Generation Parameters**:

   - `temperature=0.4`: Controls randomness and creativity in the output.

     - Lower values (0.0-0.7): More focused, deterministic, and predictable responses.
     - Higher values (0.7-1.0): More creative, diverse, and exploratory responses.
     - The setting of 0.4 prioritizes reliability over creativity.

   - `top_p=0.95`: Implements nucleus sampling to limit token selection.

     - Only considers tokens whose cumulative probability exceeds the threshold.
     - 0.95 means considering tokens that make up the top 95% of probability mass.
     - Helps prevent unlikely or inappropriate token selections.

   - `top_k=20`: Further constrains token selection to the most likely options.
     - Only the 20 most probable tokens are considered at each generation step.
     - Works in conjunction with top_p as a dual filtering system.
     - Smaller values create more predictable but potentially repetitive responses.

2. **Output Control Parameters**:

   - `candidate_count=1`: Number of separate responses to generate.

     - Useful when you want multiple options to choose from.
     - Each candidate is a complete alternative response.

   - `seed=5`: Sets a specific random seed for deterministic generation.

     - Using the same seed with identical inputs produces the same output.
     - Essential for testing, debugging, or ensuring consistent responses.

   - `max_output_tokens=100`: Limits the length of the generated response.

     - Prevents excessively verbose responses.
     - Helps control latency and cost in production applications.

   - `stop_sequences=["STOP!"]`: Defines text patterns that end generation immediately.
     - Useful for controlling output format or preventing unwanted content.
     - The model stops generating as soon as it produces the specified sequence.

3. **Repetition Control Parameters**:

   - `presence_penalty=0.0`: Reduces repetition of specific tokens.

     - Higher values (0.0-2.0) discourage the model from repeating the same content.
     - Zero means no penalty is applied.

   - `frequency_penalty=0.0`: Reduces usage of frequently occurring tokens from the prompt.
     - Higher values (0.0-2.0) encourage the model to be more original.
     - Zero means token frequency has no impact on selection probability.

4. **Parameter Combinations**:
   - These parameters can be tuned together to achieve specific behavior.
   - For creative writing: Higher temperature, lower top_k, lower frequency penalty.
   - For factual responses: Lower temperature, higher top_p, zero penalties.
   - For balanced responses: Moderate temperature (0.3-0.7), reasonable top_k and top_p.

Parameter tuning is essential for optimizing model outputs for different applications, balancing between creativity and predictability, accuracy and diversity.
