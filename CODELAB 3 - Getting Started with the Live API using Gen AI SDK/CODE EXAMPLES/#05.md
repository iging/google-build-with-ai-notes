# Example 5: Text-to-Audio Generation with Live API

This example demonstrates how to use the Live API to generate audio responses from text prompts with voice selection.

```python
import asyncio
import numpy as np
from google import genai
from google.genai.types import (
    LiveConnectConfig,
    Content,
    Part,
    SpeechConfig,
    VoiceConfig,
    PrebuiltVoiceConfig
)
from IPython.display import Audio, Markdown, display

async def text_to_audio_example():
    # Initialize the client (assuming it's already been set up)
    # client = genai.Client(vertexai=True, project=PROJECT_ID, location=LOCATION)

    # Set the model
    MODEL_ID = "gemini-2.0-flash-live-preview-04-09"

    # Select a voice
    voice_name = "Aoede"  # Options: "Aoede", "Puck", "Charon", "Kore", "Fenrir", "Leda", "Orus", "Zephyr"

    # Configure the session for audio output with the selected voice
    config = LiveConnectConfig(
        response_modalities=["AUDIO"],
        speech_config=SpeechConfig(
            voice_config=VoiceConfig(
                prebuilt_voice_config=PrebuiltVoiceConfig(
                    voice_name=voice_name,
                )
            ),
        ),
    )

    # Create the session
    async with client.aio.live.connect(
        model=MODEL_ID,
        config=config,
    ) as session:
        # Define the input text
        text_input = "Hello? Gemini are you there? Can you tell me a short story about a robot learning to paint?"
        display(Markdown(f"**Input:** {text_input}"))

        # Send the input to the model
        await session.send_client_content(
            turns=Content(role="user", parts=[Part(text=text_input)])
        )

        # Collect audio data
        audio_data = []
        async for message in session.receive():
            if (
                message.server_content.model_turn
                and message.server_content.model_turn.parts
            ):
                for part in message.server_content.model_turn.parts:
                    if part.inline_data:
                        # Convert binary audio data to numpy array
                        audio_data.append(
                            np.frombuffer(part.inline_data.data, dtype=np.int16)
                        )

        # Play the audio if data was received
        if audio_data:
            display(Markdown(f"**Audio response using voice: {voice_name}**"))
            display(Audio(np.concatenate(audio_data), rate=24000, autoplay=True))
        else:
            display(Markdown("**No audio data received**"))

# Run the example (in a notebook, you would use await text_to_audio_example())
# asyncio.run(text_to_audio_example())
```

## Explanation

This code demonstrates how to generate audio responses from text input using the Gemini Live API:

1. **Voice Selection and Configuration**:

   - Specifies a voice by name (`Aoede` in this case)
   - Available voices include: Aoede, Puck, Charon, Kore, Fenrir, Leda, Orus, and Zephyr
   - Creates a complex configuration object hierarchy:
     - `LiveConnectConfig` specifies response modality as "AUDIO"
     - `SpeechConfig` holds voice configuration settings
     - `VoiceConfig` with `PrebuiltVoiceConfig` specifies the selected voice

2. **Session Establishment**:

   - Creates an asynchronous connection to the model with the audio configuration
   - Uses context manager (`async with`) for proper session management

3. **Request Processing**:

   - Constructs and sends a text request using the same pattern as text-to-text generation
   - The model processes this text and generates an audio response

4. **Audio Data Handling**:

   - Extracts binary audio data from the response
   - Converts each audio chunk to a NumPy array using `np.frombuffer`
   - The audio data arrives in potentially multiple chunks that need to be concatenated

5. **Audio Playback**:

   - Concatenates all audio chunks using `np.concatenate`
   - Creates an interactive audio player with `IPython.display.Audio`
   - Sets sample rate to 24000 Hz (the rate used by Gemini Live API)
   - Enables autoplay for immediate playback

6. **Key Features Demonstrated**:
   - Voice customization through the configuration
   - Binary data processing for audio responses
   - Interactive audio playback in notebook environments

This example showcases how the Live API can be used for text-to-speech applications, allowing developers to create voice-enabled interfaces with customizable voices.
