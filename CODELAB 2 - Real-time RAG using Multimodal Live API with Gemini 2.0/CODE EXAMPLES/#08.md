# Example 8: Understanding LLM Limitations

This example demonstrates why RAG is necessary by showing the limitations of using an LLM without proper context.

```python
from IPython.display import Markdown, display

# Define a specific query about our fictional business
query = "What is the price of a basic tune-up at Cymbal Bikes?"

# Generate response without any context
response = client.models.generate_content(
    model=MODEL_ID,
    contents=query,
)

# Display the model's response
print("Query:", query)
print("\nModel Response:")
display(Markdown(response.text))

# Note: The correct answer should be "A basic tune-up costs $100"
# This information is in our documents but not in the model's training data
```

## Explanation

This code demonstrates the fundamental limitation of large language models that necessitates RAG implementation:

1. **Direct Query Testing**:

   - Poses a specific question about pricing at our fictional "Cymbal Bikes" shop.
   - The query asks for factual information (the price of a basic tune-up service).
   - This exact information exists in our downloaded documents but was not part of the model's training data.

2. **Ungrounded Generation Approach**:

   - Uses the `generate_content()` method to get a response directly from Gemini.
   - Provides only the model ID and query text without any additional context.
   - The model must rely solely on its pre-trained knowledge to generate a response.

3. **Observation of Model Behavior**:

   - The model typically responds in one of three ways:
     - Admitting it doesn't have specific pricing information (honest but unhelpful)
     - Providing general information about bike tune-ups (related but not answering the specific question)
     - Potentially hallucinating a price that seems plausible but is incorrect (dangerous misinformation)
   - None of these outcomes is satisfactory for a customer support application.

4. **Hallucination Problem**:

   - "Hallucination" refers to the model generating information that appears plausible but is factually incorrect.
   - This is particularly problematic in business contexts where accuracy is critical.
   - Customers rely on correct pricing information to make decisions.
   - Providing incorrect information damages business credibility and trust.

5. **The Case for RAG**:
   - This demonstration establishes why simple LLM generation is insufficient for domain-specific questions.
   - It shows the need for retrieving and providing relevant context to the model.
   - RAG addresses this limitation by finding specific information in documents and using it to ground the model's response.
   - The goal is to replace uncertainty or hallucination with factual answers derived directly from authoritative sources.

This example sets up the foundation for understanding why RAG is necessary and valuable, especially in business applications requiring factual accuracy.
